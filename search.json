[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "core.html#load_audio",
    "href": "core.html#load_audio",
    "title": "core",
    "section": "load_audio",
    "text": "load_audio\nWe’ll start with a basic utilty to read an audio file. If it’s not at the sample rate we want, we’ll automatically resample it. Note that if you want MP3 support, you’ll need to install ffmpeg system-wide first.\n\nsource\n\nload_audio\n\n load_audio (filename:str, sr=48000, verbose=True, norm='')\n\nthis loads an audio file as a torch tensor\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfilename\nstr\n\nname of file to load\n\n\nsr\nint\n48000\nsample rate in Hz\n\n\nverbose\nbool\nTrue\nwhether or not to print notices of resampling\n\n\nnorm\nstr\n\npassedto normalize_audio(), see above\n\n\nReturns\ntensor\n\n\n\n\n\nUsing the file in examples/, let’s see how this works:\n\naudio = load_audio('examples/example.wav')\n\nResampling examples/example.wav from 44100 Hz to 48000 Hz\n\n\n\naudio = load_audio('examples/example.wav',verbose=False)\n\nLet’s check to see if we can read MP3s (assuming ffmpeg is installed):\n\nfor norm in ['','global','channel']:\n    audio = load_audio('examples/stereo_pewpew.mp3',verbose=False, norm=norm)\n    print(f\"norm = {norm}: shape = \",audio.shape, \"Per-channel maxes are : \", audio.numpy().max(axis=-1))\n\nnorm = : shape =  torch.Size([2, 234505]) Per-channel maxes are :  [0.8509471 0.4230432]\nnorm = global: shape =  torch.Size([2, 234505]) Per-channel maxes are :  [0.99      0.4921725]\nnorm = channel: shape =  torch.Size([2, 234505]) Per-channel maxes are :  [0.99       0.83469504]"
  },
  {
    "objectID": "core.html#get_dbmax",
    "href": "core.html#get_dbmax",
    "title": "core",
    "section": "get_dbmax",
    "text": "get_dbmax\nFinds loudest sample value in the entire clip and returns the value as decibels\n\nsource\n\nget_dbmax\n\n get_dbmax (audio)\n\nfinds the loudest value in the entire clip and puts that into dBs\n\n\n\n\nDetails\n\n\n\n\naudio\ntorch tensor of (multichannel) audio"
  },
  {
    "objectID": "core.html#is_silence",
    "href": "core.html#is_silence",
    "title": "core",
    "section": "is_silence",
    "text": "is_silence\nSometimes we’ll want to know if a file is “silent”, i.e. if its contents are quieter than some threshold. Here’s one simple way to implement that:\n\nsource\n\naudio_float_to_int\n\n audio_float_to_int (waveform)\n\nconverts torch float to numpy int16 (for playback in notebooks)\n\nprint(audio.dtype)\nprint(audio_float_to_int(audio).dtype)\n\ntorch.float32\nint16\n\n\n\nsource\n\n\nis_silence\n\n is_silence (audio, thresh=-60)\n\nchecks if entire clip is ‘silence’ below some dB threshold\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naudio\n\n\ntorch tensor of (multichannel) audio\n\n\nthresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\n\nLet’s test that with some tests. If all goes well, the following assert statements will all pass uneventfully.\n\nx = torch.ones((2,10))\nassert not is_silence(1e-3*x) # not silent\nassert is_silence(1e-5*x) # silent\nassert is_silence(1e-3*x, thresh=-50) # higher thresh"
  },
  {
    "objectID": "core.html#batch_it_crazy",
    "href": "core.html#batch_it_crazy",
    "title": "core",
    "section": "batch_it_crazy",
    "text": "batch_it_crazy\nThis is a pretty basic utility for breaking up a long sequence into batches, e.g. for model inference\n\nsource\n\nbatch_it_crazy\n\n batch_it_crazy (x, win_len)\n\n(pun intended) Chop up long sequence into a batch of win_len windows\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nx\na time series as a PyTorch tensor, e.g. stereo or mono audio\n\n\nwin_len\nlength of each “window”, i.e. length of each element in new batch\n\n\n\nTesting batch_it_crazy() for stereo input:\n\nx = torch.ones([2,1000])  # stereo\nbatch_it_crazy(x, 10).shape\n\ntorch.Size([101, 2, 10])\n\n\n…and for mono:\n\nx = torch.ones([1000])   # mono\nbatch_it_crazy(x, 10).shape\n\ntorch.Size([101, 1, 10])\n\n\n…and yeah, currently that “1,” stays because other parts of the code(s) will be assuming “multichannel” audio."
  },
  {
    "objectID": "core.html#makedir",
    "href": "core.html#makedir",
    "title": "core",
    "section": "makedir",
    "text": "makedir\nThe next routine creates a directory if it doesn’t already exist. We’ll even let it take a “nested” directory such as a/b/c/d and the routine will create any directories in that string.\n\nsource\n\nmakedir\n\n makedir (path:str)\n\ncreates directories where they don’t exist\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\ndirectory or nested set of directories"
  },
  {
    "objectID": "core.html#get_audio_filenames",
    "href": "core.html#get_audio_filenames",
    "title": "core",
    "section": "get_audio_filenames",
    "text": "get_audio_filenames\nOften we’ll want to grab a long list of audio filenames by looking through a directory and all its subdirectories. We could use something like glob, glob turns out to be extremely slow when large numbers of files (say, more than 100,000) are involved. Instead we will use the much faster os.scandir(), which was packaged nicely into the following routine in an answer to a StackOverflow question from which this code is modified:\n\nsource\n\nfast_scandir\n\n fast_scandir (dir:str, ext:list)\n\nvery fast glob alternative. from https://stackoverflow.com/a/59803793/4259243\n\n\n\n\nType\nDetails\n\n\n\n\ndir\nstr\ntop-level directory at which to begin scanning\n\n\next\nlist\nlist of allowed file extensions\n\n\n\nQuick test:\n\n_, files = fast_scandir('.', ['wav','flac','ogg','aiff','aif','mp3'])\nfiles\n\n['./audio_out.wav', './examples/stereo_pewpew.mp3', './examples/example.wav']\n\n\nOften, rather than being given a single parent directory, we may be given a list of directories in which to look for files. The following just called fast_scandir() for each of those:\n\nsource\n\n\nget_audio_filenames\n\n get_audio_filenames (paths:list)\n\nrecursively get a list of audio filenames\n\n\n\n\nType\nDetails\n\n\n\n\npaths\nlist\ndirectories in which to search\n\n\n\nHere’s a fun trick to show off how fast this is: Run in the user’s directory tree:\n\npath = str(os.path.expanduser(\"~\"))+'/Downloads'\nif os.path.exists(path):\n    files = get_audio_filenames(path)\n    print(f\"Found {len(files)} audio files.\")\nelse:\n    print(\"Ok it was just a thought.\")\n\nFound 106148 audio files.\n\n\n\nsource\n\n\nuntuple\n\n untuple (x, verbose=False)\n\nRecursive. For when you’re sick of tuples and lists: keeps peeling off elements until we get a non-tuple or non-list, i.e., returns the ‘first’ data element we can ‘actually use’\n\na = [[((5,6),7)]]\nprint(a)\nprint(untuple(a, verbose=True))\n\n[[((5, 6), 7)]]\nyea: x =  [[((5, 6), 7)]]\nyea: x =  [((5, 6), 7)]\nyea: x =  ((5, 6), 7)\nyea: x =  (5, 6)\nno: x =  5\n5"
  },
  {
    "objectID": "spectrofu.html",
    "href": "spectrofu.html",
    "title": "spectrofu",
    "section": "",
    "text": "Assumes pre-chunking e.g. via chunkadelic — This is pretty much a simplified duplicate of chunkadelic.\nNote: Duplicates the directory structure(s) referenced by input paths.\n\nsource\n\nsave_stft\n\n save_stft (audio:<built-inmethodtensoroftypeobjectat0x7f99dfb389c0>,\n            new_filename:str)\n\ncoverts audio to stft image and saves it\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\naudio\ntensor\nlong audio file to be chunked\n\n\nnew_filename\nstr\nstem of new filename(s) to be output as spectrogram images\n\n\n\n\nsource\n\n\nmain\n\n main ()\n\n\nsource\n\n\nprocess_one_file\n\n process_one_file (filenames:list, args, file_ind)\n\nthis turns one audio file into a spectrogram. left channel only for now\n\n\n\n\nType\nDetails\n\n\n\n\nfilenames\nlist\nlist of filenames from which we’ll pick one\n\n\nargs\n\noutput of argparse\n\n\nfile_ind\n\nindex from filenames list to read from\n\n\n\n\n! spectrofu -h\n\nusage: spectrofu [-h] [--sr SR] [--workers WORKERS]\n                 output_path input_paths [input_paths ...]\n\npositional arguments:\n  output_path        Path of output for spectrogram-ified data\n  input_paths        Path(s) of a file or a folder of files. (recursive)\n\noptional arguments:\n  -h, --help         show this help message and exit\n  --sr SR            Output sample rate (default: 48000)\n  --workers WORKERS  Maximum number of workers to use (default: all) (default:\n                     14)"
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "Originally written for https://github.com/zqevans/audio-diffusion/blob/main/viz/viz.py\nsource"
  },
  {
    "objectID": "viz.html#d-scatter-plots",
    "href": "viz.html#d-scatter-plots",
    "title": "viz",
    "section": "3D Scatter plots",
    "text": "3D Scatter plots\nTo visualize point clouds in notebook and on WandB:\n\n\nsource\n\npca_point_cloud\n\n pca_point_cloud (tokens, color_scheme='batch', output_type='wandbobj',\n                  mode='markers', size=3, line={'color':\n                  'rgba(10,10,10,0.01)'})\n\nreturns a 3D point cloud of the tokens using PCA\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokens\n\n\nembeddings / latent vectors. shape = (b, d, n)\n\n\ncolor_scheme\nstr\nbatch\n‘batch’: group by sample, otherwise color sequentially\n\n\noutput_type\nstr\nwandbobj\nplotly | points | wandbobj. NOTE: WandB can do ‘plotly’ directly!\n\n\nmode\nstr\nmarkers\nplotly scatter mode. ‘lines+markers’ or ‘markers’\n\n\nsize\nint\n3\nsize of the dots\n\n\nline\ndict\n{‘color’: ‘rgba(10,10,10,0.01)’}\nif mode=‘lines+markers’, plotly line specifier. cf. https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.scatter3d.html#plotly.graph_objects.scatter3d.Line\n\n\n\nTo display in the notebook (and the online documenation), we need a bit of extra code:\n\nsource\n\n\nshow_pca_point_cloud\n\n show_pca_point_cloud (tokens, color_scheme='batch', mode='markers',\n                       line={'color': 'rgba(10,10,10,0.01)'})\n\ndisplay a 3d scatter plot of tokens in notebook\n\nsource\n\n\nsetup_plotly\n\n setup_plotly (nbdev=True)\n\nPlotly is already ‘setup’ on colab, but on regular Jupyter notebooks we need to do a couple things\n\nsource\n\n\non_colab\n\n on_colab ()\n\nReturns true if code is being executed on Colab, false otherwise\nTest the point cloud viz inside a notebook:\n\ntokens = torch.rand((16,32,152))\nshow_pca_point_cloud(tokens)  # default, no lines connecting dots\n\n\n\n\n\n                                                \n\n\nOr we can add lines connecting the dots, such as a faint gray line:\n\nshow_pca_point_cloud(tokens, mode='lines+markers')"
  },
  {
    "objectID": "viz.html#print-audio-info",
    "href": "viz.html#print-audio-info",
    "title": "viz",
    "section": "Print audio info",
    "text": "Print audio info\n\nsource\n\nprint_stats\n\n print_stats (waveform, sample_rate=None, src=None, print=<built-in\n              function print>)\n\nprint stats about waveform. Taken verbatim from pytorch docs.\nTesting that:\n\naudio_filename = 'examples/example.wav'\nwaveform = load_audio(audio_filename)\nprint_stats(waveform)\n\nResampling examples/example.wav from 44100 Hz to 48000 Hz\nShape: (1, 55728)\nDtype: torch.float32\n - Max:      0.647\n - Min:     -0.647\n - Mean:     0.000\n - Std Dev:  0.075\n\ntensor([[-3.0239e-04, -3.8517e-04, -6.0043e-04,  ...,  2.4789e-05,\n         -1.3458e-04, -8.0428e-06]])"
  },
  {
    "objectID": "viz.html#spectrograms",
    "href": "viz.html#spectrograms",
    "title": "viz",
    "section": "Spectrograms",
    "text": "Spectrograms\n\nsource\n\nmel_spectrogram\n\n mel_spectrogram (waveform, power=2.0, sample_rate=48000, db=False,\n                  n_fft=1024, n_mels=128, debug=False)\n\ncalculates data array for mel spectrogram (in however many channels)\n\nsource\n\n\nspectrogram_image\n\n spectrogram_image (spec, title=None, ylabel='freq_bin', aspect='auto',\n                    xmax=None, db_range=[35, 120], justimage=False)\n\nModified from PyTorch tutorial https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\n\nsource\n\n\naudio_spectrogram_image\n\n audio_spectrogram_image (waveform, power=2.0, sample_rate=48000,\n                          print=<built-in function print>, db=False,\n                          db_range=[35, 120], justimage=False, log=False)\n\nWrapper for calling above two routines at once, does Mel scale; Modified from PyTorch tutorial https://pytorch.org/tutorials/beginner/audio_feature_extractions_tutorial.html\nLet’s test the above routine:\n\nspec_graph = audio_spectrogram_image(waveform, justimage=False, db=False, db_range=[-60,20])\ndisplay(spec_graph)\n\n/Users/shawley/opt/anaconda3/envs/shazbot/lib/python3.8/site-packages/torchaudio/functional/functional.py:571: UserWarning:\n\nAt least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n\n\n\n\n\n\n\n\n‘Playable Spectrograms’\nSource(s): Original code by Scott Condron (@scottire) of Weights and Biases, edited by @drscotthawley\ncf. @scottire’s original code here: https://gist.github.com/scottire/a8e5b74efca37945c0f1b0670761d568\nand Morgan McGuire’s edit here; https://github.com/morganmcg1/wandb_spectrogram\n\nint(np.random.rand()*10000)\n\n8890\n\n\n\nsource\n\n\nplayable_spectrogram\n\n playable_spectrogram (waveform, sample_rate=48000, specs:str='all',\n                       layout:str='row', height=170, width=400,\n                       cmap='viridis', output_type='wandb', debug=True)\n\nTakes a tensor input and returns a [wandb.]HTML object with spectrograms of the audio specs : “all_specs”, spetrograms only “all”, all plots “melspec”, melspectrogram only “spec”, spectrogram only “wave_mel”, waveform and melspectrogram only “waveform”, waveform only, equivalent to wandb.Audio object\nLimitations: spectrograms show channel 0 only (i.e., mono)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nwaveform\n\n\naudio, PyTorch tensor\n\n\nsample_rate\nint\n48000\nsample rate in Hz\n\n\nspecs\nstr\nall\nsee docstring below\n\n\nlayout\nstr\nrow\n‘row’ or ‘grid’\n\n\nheight\nint\n170\nheight of spectrogram image\n\n\nwidth\nint\n400\nwidth of spectrogram image\n\n\ncmap\nstr\nviridis\ncolormap string for Holoviews, see https://holoviews.org/user_guide/Colormaps.html\n\n\noutput_type\nstr\nwandb\n‘wandb’, ‘html_file’, ‘live’: use live for notebooks\n\n\ndebug\nbool\nTrue\nflag for internal print statements\n\n\n\n\nsource\n\n\ngenerate_melspec\n\n generate_melspec (audio_data, sample_rate=48000, power=2.0, n_fft=1024,\n                   win_length=None, hop_length=None, n_mels=128)\n\nhelper routine for playable_spectrogram\nSample usage with WandB:\nwandb.init(project='audio_test')\nwandb.log({\"playable_spectrograms\": playable_spectrogram(waveform)})\nwandb.finish()\nSee example result at https://wandb.ai/drscotthawley/playable_spectrogram_test/\nTest the playable spectrogram:\n\nHTML(playable_spectrogram(waveform, output_type='html_file'))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n      \n      Panel\n              \n            \n            \n            \n            \n            \n            \n            \n            \n\n          \n  \n  \n          \n              \n      \n        \n        \n  \n\n\n\nLet’s show off the multichannel waveform display:\n\nmc_wave = load_audio('examples/stereo_pewpew.mp3')\nplayable_spectrogram(mc_wave, specs='wave_mel', output_type='live')\n\nResampling examples/stereo_pewpew.mp3 from 22050 Hz to 48000 Hz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nplayable_spectrogram(waveform, specs=\"waveform\", output_type='live')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nsource\n\n\ntokens_spectrogram_image\n\n tokens_spectrogram_image (tokens, aspect='auto', title='Embeddings',\n                           ylabel='index')\n\nfor visualizing embeddings in a spectrogram-like way\n\ntokens_spectrogram_image(tokens)\n\n\nsource\n\n\nplot_jukebox_embeddings\n\n plot_jukebox_embeddings (zs, aspect='auto')\n\nmakes a plot of jukebox embeddings"
  },
  {
    "objectID": "chunkadelic.html",
    "href": "chunkadelic.html",
    "title": "chunkadelic",
    "section": "",
    "text": "Note: Duplicates the directory structure(s) referenced by input paths.\n\nsource\n\nblow_chunks\n\n blow_chunks (audio:<built-inmethodtensoroftypeobjectat0x7f99dfb389c0>,\n              new_filename:str, chunk_size:int, sr=48000, norm=False,\n              spacing=0.5, strip=False, thresh=-70)\n\nchunks up the audio and saves them with –{i} on the end of each chunk filename\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naudio\ntensor\n\nlong audio file to be chunked\n\n\nnew_filename\nstr\n\nstem of new filename(s) to be output as chunks\n\n\nchunk_size\nint\n\nhow big each audio chunk is, in samples\n\n\nsr\nint\n48000\naudio sample rate in Hz\n\n\nnorm\nbool\nFalse\nnormalize input audio, based on the max of the absolute value [global/channel]\n\n\nspacing\nfloat\n0.5\nfraction of each chunk to advance between hops\n\n\nstrip\nbool\nFalse\nstrip silence: chunks with max power in dB below this value will not be saved to files\n\n\nthresh\nint\n-70\nthreshold in dB for determining what counts as silence\n\n\n\n\nsource\n\n\nprocess_one_file\n\n process_one_file (filenames:list, args, file_ind)\n\nthis chunks up one file\n\n\n\n\nType\nDetails\n\n\n\n\nfilenames\nlist\nlist of filenames from which we’ll pick one\n\n\nargs\n\noutput of argparse\n\n\nfile_ind\n\nindex from filenames list to read from\n\n\n\n\nsource\n\n\nmain\n\n main ()\n\n\n! chunkadelic -h\n\nusage: chunkadelic [-h] [--chunk_size CHUNK_SIZE] [--sr SR] [--norm False]\n                   [--spacing SPACING] [--strip] [--thresh THRESH]\n                   [--workers WORKERS] [--nomix]\n                   output_path input_paths [input_paths ...]\n\npositional arguments:\n  output_path           Path of output for chunkified data\n  input_paths           Path(s) of a file or a folder of files. (recursive)\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --chunk_size CHUNK_SIZE\n                        Length of chunks (default: 131072)\n  --sr SR               Output sample rate (default: 48000)\n  --norm False          Normalize audio, based on the max of the absolute\n                        value [global/channel/False] (default: False)\n  --spacing SPACING     Spacing factor, advance this fraction of a chunk per\n                        copy (default: 0.5)\n  --strip               Strips silence: chunks with max dB below <thresh> are\n                        not outputted (default: False)\n  --thresh THRESH       threshold in dB for determining what constitutes\n                        silence (default: -70)\n  --workers WORKERS     Maximum number of workers to use (default: all)\n                        (default: 5)\n  --nomix               (BDCT Dataset specific) exclude output of \"*/Audio\n                        Files/*Mix*\" (default: False)"
  },
  {
    "objectID": "hpc.html",
    "href": "hpc.html",
    "title": "hpc",
    "section": "",
    "text": "This part isn’t strictly for audio i/o, but is nevertheless a normal part of Harmonai’s operations. The point of this package is to reduce code-copying between Harmonai projects.\nHeads up: Huggingface accelerate support will likely be deprecated soon. We found accelerate necessary because of problems running PyTorch Lightning on multiple nodes, but those problems have now been resolved. Thus we will likely be using Lighting, so you will see that dependency being added and perhaps accelerate being removed.\nsource"
  },
  {
    "objectID": "hpc.html#pytorchaccelerate-model-routines",
    "href": "hpc.html#pytorchaccelerate-model-routines",
    "title": "hpc",
    "section": "PyTorch+Accelerate Model routines",
    "text": "PyTorch+Accelerate Model routines\nFor when the model is wrapped in a accelerate accelerator\n\nsource\n\nsave\n\n save (accelerator, args, model, opt=None, epoch=None, step=None)\n\nfor checkpointing & model saves\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naccelerator\n\n\nHuggingface accelerator object\n\n\nargs\n\n\nprefigure args dict, (we only use args.name)\n\n\nmodel\n\n\nthe model, pre-unwrapped\n\n\nopt\nNoneType\nNone\noptimizer state\n\n\nepoch\nNoneType\nNone\ntraining epoch number\n\n\nstep\nNoneType\nNone\ntraining setp number\n\n\n\n\nsource\n\n\nload\n\n load (accelerator, model, filename:str, opt=None)\n\nload a saved model checkpoint\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\naccelerator\n\n\nHuggingface accelerator object\n\n\nmodel\n\n\nan uninitialized model (pre-unwrapped) whose weights will be overwritten\n\n\nfilename\nstr\n\nname of the checkpoint file\n\n\nopt\nNoneType\nNone\noptimizer state UNUSED FOR NOW"
  },
  {
    "objectID": "hpc.html#utils-for-accelerate-or-lightning",
    "href": "hpc.html#utils-for-accelerate-or-lightning",
    "title": "hpc",
    "section": "Utils for Accelerate or Lightning",
    "text": "Utils for Accelerate or Lightning\nBe sure to use “unwrap” any accelerate model when calling these\n\nsource\n\nn_params\n\n n_params (module)\n\nReturns the number of trainable parameters in a module. Be sure to use accelerator.unwrap_model when calling this.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nmodule\nraw PyTorch model/module, e.g. returned by accelerator.unwrap_model()\n\n\n\n\nsource\n\n\nfreeze\n\n freeze (model)\n\nfreezes model weights; turns off gradient info If using accelerate, call thisaccelerator.unwrap_model when calling this.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nmodel\nraw PyTorch model, e.g. returned by accelerator.unwrap_model()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "aeiou",
    "section": "",
    "text": "Pronounced “ayoo”"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "aeiou",
    "section": "Install",
    "text": "Install\nIt is recommended you install the latest version from GitHub via\npip install git+https://github.com/drscotthawley/aeiou.git\nHowever binaries will be occasionally updated on PyPI, installed via\npip install aeiou"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "aeiou",
    "section": "How to use",
    "text": "How to use\nThis is a series of utility routines developed in support of multiple projects within the Harmonai organization. See individual documentation pages for more specific instructions on how these can be used. Note that this is research code, so it’s a) in flux and b) in need of improvements to documenation."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "aeiou",
    "section": "Documentation",
    "text": "Documentation\nDocumentation for this library is hosted on the aeiou GitHub Pages site."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "aeiou",
    "section": "Contributing",
    "text": "Contributing\nContributions are welcome – especially for improvements to documentation! To contribute:\n\nFork this repo and then clone your fork to your local machine.\nCreate a new (local) branch: git -b mybranch (or whatever you want to call it).\nThis library is written entirely in nbdev version 2, using Jupyter notebooks.\nInstall nbdev and then you can edit the Jupyter notebooks.\nAfter editing notebooks, run nbdev_prepare\nIf that succeeds, you can do git add *.ipynb aeiou/*.py; git commit and then git push to get your changes to back to your fork on GitHub.\nThen send a Pull Request from your fork to the main aeiou repository."
  },
  {
    "objectID": "index.html#attribution",
    "href": "index.html#attribution",
    "title": "aeiou",
    "section": "Attribution",
    "text": "Attribution\n@misc{aeiou,\n  author = {Scott H. Hawley},\n  title = {aeiou: audio engineering i/o utilities},\n  year  = {2022},\n  url   = {https://github.com/drscotthawley/aeiou},\n}\nCopyright (c) Scott H. Hawley, 2022."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "aeiou",
    "section": "License",
    "text": "License\nLicense is APACHE 2.0."
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "datasets",
    "section": "",
    "text": "Many of these routines are dupes or mods from “audio-diffusion” repo by Zach Evans w/ contributions by Scott Hawley https://github.com/zqevans/audio-diffusion/blob/main/diffusion/utils.py"
  },
  {
    "objectID": "datasets.html#augmentation-routines",
    "href": "datasets.html#augmentation-routines",
    "title": "datasets",
    "section": "Augmentation routines",
    "text": "Augmentation routines\nNot all of these are used. Code copied from https://github.com/zqevans/audio-diffusion/blob/main/diffusion/utils.py\n\nsource\n\nPadCrop\n\n PadCrop (n_samples, randomize=True, redraw_silence=True,\n          silence_thresh=-60, max_redraws=2)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_samples\n\n\nlength of chunk to extract from longer signal\n\n\nrandomize\nbool\nTrue\ndraw cropped chunk from a random position in audio file\n\n\nredraw_silence\nbool\nTrue\na chunk containing silence will be replaced with a new one\n\n\nsilence_thresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\nmax_redraws\nint\n2\nwhen redrawing silences, don’t do it more than this many\n\n\n\n\nsource\n\n\nPhaseFlipper\n\n PhaseFlipper (p=0.5)\n\nshe was PHAAAAAAA-AAAASE FLIPPER, a random invert yeah\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.5\nprobability that phase flip will be applied\n\n\n\n\nsource\n\n\nFillTheNoise\n\n FillTheNoise (p=0.33)\n\nrandomly adds a bit of noise, just to spice things up\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n0.33\nprobability that noise will be added\n\n\n\n\nsource\n\n\nRandPool\n\n RandPool (p=0.2)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nNormInputs\n\n NormInputs (do_norm=True)\n\nNormalize inputs to [-1,1]. Useful for quiet inputs\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndo_norm\nbool\nTrue\ncontrollable parameter for turning normalization on/off\n\n\n\n\nsource\n\n\nMono\n\n Mono ()\n\nconvert audio to mono\n\nsource\n\n\nStereo\n\n Stereo ()\n\nconvert audio to stereo\n\nsource\n\n\nRandomGain\n\n RandomGain (min_gain, max_gain)\n\napply a random gain to audio"
  },
  {
    "objectID": "datasets.html#webdataset-support",
    "href": "datasets.html#webdataset-support",
    "title": "datasets",
    "section": "WebDataset support",
    "text": "WebDataset support\n\nBackground Info\nRefer to the official WebDataset Repo on GitHub.\n\nWebDataset makes it easy to write I/O pipelines for large datasets. Datasets can be stored locally or in the cloud.\n\nThey use the word “shards” but never define what “shard” means. I (S.H.) surmise they mean the groups of data files which are gathered into a series of .tar files – the .tar files are the shards?\ncf. Video Tutorial: “Loading Training Data with WebDataset”.\nThe recommended usage for AWS S3 can be seen in [this GitHub Issue comment by tmbdev] (https://github.com/webdataset/webdataset/issues/21#issuecomment-706008342):\nurl = \"pipe:s3cmd get s3://bucket/dataset-{000000..000999}.tar -\"\ndataset = wds.Dataset(url)...\n\n1 s3cmd get should read aws s3 cp.\n\nThat URL is expecting a contiguously-numbered range of .tar files. So if the file numbers are contiguous (no gaps), then we’ll have an easy time. Otherwise, there are ways to pass in a long list of similar “pipe:…tar” ‘urls’ for each and every tar file, which is still not a big deal though it may appear messier.\n\nNOTE: be prepared for extensive ‘testing cases’ shown for the following routines.\n\n\n\nGeneral utility: get_s3_contents()\n\nsource\n\n\nget_s3_contents\n\n get_s3_contents (dataset_path, s3_url_prefix='s3://s-laion-\n                  audio/webdataset_tar/', filter='', recursive=True,\n                  debug=False)\n\nGets a list of names of files or subdirectories on an s3 path\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset_path\n\n\n“name” of the dataset on s3\n\n\ns3_url_prefix\nstr\ns3://s-laion-audio/webdataset_tar/\ns3 bucket to check\n\n\nfilter\nstr\n\nonly grab certain filename / extensions\n\n\nrecursive\nbool\nTrue\ncheck all subdirectories. RECOMMEND LEAVING THIS TRUE\n\n\ndebug\nbool\nFalse\nprint debugging info (don’t rely on this info staying consistent)\n\n\n\nLet’s test that on the FSD50K dataset: > Note: These tests will only work on systems on which you have valid AWS credentials, e.g. the Stability cluster. If the docs show a bunch of blanks in what follows, that’s why.\n\nget_s3_contents('130000_MIDI_SONGS', recursive=False)\n\n['130000_Pop_Rock_Classical_Videogame_EDM_MIDI_Archive[6_19_15]/',\n '2/',\n '3/',\n '4/',\n '5/',\n '6/',\n '7/',\n '8/',\n '9/',\n 'A/',\n 'AMERICANA_FOLK_www.pdmusic.org_MIDIRip/',\n 'Arabic and Tribal Rhythms/',\n 'B/',\n 'Classical Archives - The Greats (MIDI)/',\n 'Classical_Guitar_classicalguitarmidi.com_MIDIRip/',\n 'Classical_Piano_piano-midi.de_MIDIRip/',\n 'Classical_Violin_theviolinsite.com_MIDIRip/',\n 'Classical_mfiles.co.uk_MIDIRip/',\n 'Classical_www.midiworld.com_MIDIRip/',\n 'D/',\n 'Dub_MIDIRip/',\n 'E/',\n 'F/',\n 'G/',\n 'Guitar_midkar.com_MIDIRip/',\n 'H/',\n 'I/',\n 'J/',\n 'Jazz_www.thejazzpage.de_MIDIRip/',\n 'K/',\n 'L/',\n 'M/',\n 'Metal_Rock_rock.freemidis.net_MIDIRip/',\n 'Metal_Rock_wolverine-metalmidi.wen.ru_MIDIRip/',\n 'N/',\n 'O/',\n 'P/',\n 'Q/',\n 'Ragtime_rtpress.com_MIDIRip/',\n 'S/',\n 'T/',\n 'TV_Themes_www.tv-timewarp.co.uk_MIDIRip/',\n 'U/',\n 'V/',\n 'W/',\n 'X/',\n 'Y/',\n 'Z/']\n\n\n\nget_s3_contents('FSD50K/test/')\n\n['0.tar',\n '1.tar',\n '10.tar',\n '11.tar',\n '12.tar',\n '13.tar',\n '14.tar',\n '15.tar',\n '16.tar',\n '17.tar',\n '18.tar',\n '19.tar',\n '2.tar',\n '3.tar',\n '4.tar',\n '5.tar',\n '6.tar',\n '7.tar',\n '8.tar',\n '9.tar',\n 'sizes.json']\n\n\nAnd let’s try filtering for only tar files:\n\ntar_names = get_s3_contents('FSD50K/test', filter='tar')\ntar_names\n\n['0.tar',\n '1.tar',\n '10.tar',\n '11.tar',\n '12.tar',\n '13.tar',\n '14.tar',\n '15.tar',\n '16.tar',\n '17.tar',\n '18.tar',\n '19.tar',\n '2.tar',\n '3.tar',\n '4.tar',\n '5.tar',\n '6.tar',\n '7.tar',\n '8.tar',\n '9.tar']\n\n\nList all LAION audio datasets:\n\nget_s3_contents('',recursive=False)\n\n['130000_MIDI_SONGS/',\n 'BBCSoundEffects/',\n 'CREMA-D/',\n 'Clotho/',\n 'CoVoST_2/',\n 'ESC50_1/',\n 'ESC50_2/',\n 'ESC50_3/',\n 'ESC50_4/',\n 'ESC50_5/',\n 'EmoV_DB/',\n 'Europarl-st/',\n 'FMA/',\n 'FMA_updated/',\n 'FSD50K/',\n 'LJSpeech/',\n 'LibriSpeech/',\n 'MACS/',\n 'Urbansound8K/',\n 'VGGSound/',\n 'VocalSketch/',\n 'WavText5K/',\n 'YT_dataset/',\n 'ZAPSPLAT/',\n 'audiocaps/',\n 'audioset/',\n 'audioset_balanced_train_t5/',\n 'audioset_eval_t5/',\n 'audioset_strong/',\n 'audioset_t5/',\n 'audioset_unbalanced_train_t5/',\n 'audiostock/',\n 'cambridge_dictionary/',\n 'clotho_mixed/',\n 'epidemic_sound_effects/',\n 'epidemic_sound_effects_t5/',\n 'esc50/',\n 'esc50_no_overlap/',\n 'fine_grained_vocal_imitation_set/',\n 'free_to_use_sounds/',\n 'freesound/',\n 'freesound_no_overlap/',\n 'freesound_no_overlap_noesc50/',\n 'fsd50k_200_class_label/',\n 'fsd50k_class_label/',\n 'librispeech_asr/',\n 'midi50k/',\n 'million_song_dataset/',\n 'mswc/',\n 'musicnet/',\n 'paramount_motion/',\n 'ravdess/',\n 'sonniss_game_effects/',\n 'tmp_eval/',\n 'urbansound8k_class_label/',\n 'usd8k_no_overlap/',\n 'wesoundeffects/']\n\n\nList all of dadabots’ ekto tar files in their 1/ directory:\n\ncontents = get_s3_contents('ekto/1/', s3_url_prefix='s3://s-harmonai/datasets/', recursive=True, debug=False)\ncontents[0:10] # let's just print 10 of them\n\n['ekto-000000.tar',\n 'ekto-000001.tar',\n 'ekto-000002.tar',\n 'ekto-000003.tar',\n 'ekto-000004.tar',\n 'ekto-000005.tar',\n 'ekto-000006.tar',\n 'ekto-000007.tar',\n 'ekto-000008.tar',\n 'ekto-000009.tar']\n\n\n\n\nFor contiguous file-number lists…\nMaybe the range of tar numbers is contigous. (In the LAION AudoiDataset archives, they are each contiguous within train/, valid/, and test/ subsets.) If so, let’s have something to output that range:\n\nsource\n\n\nget_contiguous_range\n\n get_contiguous_range (tar_names)\n\ngiven a string of tar file names, return a string of their range if the numbers are contiguous. Otherwise return empty string\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ntar_names\nlist of tar file names, although the .tar part is actually optional\n\n\n\n\ncont_range = get_contiguous_range(tar_names)\ncont_range\n\n'{0..19}'\n\n\nTest if leading zeros are preserved:\n\nget_contiguous_range(['0000'+x for x in tar_names])\n\n'{00000..000019}'\n\n\nTest zero-element and single element versions:\n\nprint(get_contiguous_range([]))\nprint(get_contiguous_range([1]))\n\n\n1\n\n\nAnd show that ‘.tar’ is optional:\n\nget_contiguous_range(['01','02','3'])\n\n'{01..3}'\n\n\n….So, if a contiguous range of tar file names is available in a WebDataset directory, then we can just use the native WebDataset creation utilities and can ignore all the other %$#*& that’s about to follow below.\nLet’s test the simple version first:\n\ns3_url_prefix='s3://s-laion-audio/webdataset_tar/'\nurl = f\"pipe:aws s3 cp {s3_url_prefix}FSD50K/test/{cont_range}.tar -\"  # 'aws get' is not a thing. 'aws cp' is\nprint(url)\ndataset = wds.WebDataset(url)\n\npipe:aws s3 cp s3://s-laion-audio/webdataset_tar/FSD50K/test/{0..19}.tar -\n\n\nWebDataset is a kind of IterableDataset, so we can iterate over it directly:\n\n## NOTE TO SELF: DON'T RUN THIS ON STABILITY CLUSTER HEADNODE (But Jupyter nodes are fine)\nfor sample in dataset:  \n    for k,v in sample.items():  # print the all entries in dict\n        print(f\"{k:20s} {repr(v)[:50]}\")\n    break                       # abort after first dict\n\n__key__              './mnt/audio_clip/processed_datasets/FSD50K/test/3\n__url__              'pipe:aws s3 cp s3://s-laion-audio/webdataset_tar/\nflac                 b'fLaC\\x00\\x00\\x00\"\\x12\\x00\\x12\\x00\\x00\\x0ee\\x00\\x\njson                 b'{\"text\": [\"The sounds of Aircraft, Engine, Fixed\n\n\n\naudio_keys = (\"skeez\",\"jop\")\nfound_key, rewrite_key = '', ''\nfor k,v in sample.items():  # print the all entries in dict\n    for akey in audio_keys:\n        if k.endswith(akey): \n            found_key, rewrite_key = k, akey\n            break\n    if '' != found_key: break \nif '' == found_key:  # got no audio!   \n    print(\"Error: No audio in this sample:\")\n    for k,v in sample.items():  # print the all entries in dict\n        print(f\"{k:20s} {repr(v)[:50]}\")\n\nError: No audio in this sample:\n__key__              './mnt/audio_clip/processed_datasets/FSD50K/test/3\n__url__              'pipe:aws s3 cp s3://s-laion-audio/webdataset_tar/\nflac                 b'fLaC\\x00\\x00\\x00\"\\x12\\x00\\x12\\x00\\x00\\x0ee\\x00\\x\njson                 b'{\"text\": [\"The sounds of Aircraft, Engine, Fixed\n\n\nThere’s a built-in decoder for various audio formats, so we can just use:\n\ndataset = wds.WebDataset(url).decode(wds.torch_audio) # throw out the json\nsample = next(iter(dataset))\naudio, sr = (sample[\"flac\"])\nplayable_spectrogram(audio, specs='wave_mel', output_type='live')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLet’s check the contents of the georgeblood Internet Archive:\n\n#urls = get_all_s3_urls(names=['georgeblood'], subsets=[], s3_url_prefix='s3://iarchive-stability/')\nurls = ['pipe:aws s3 cp s3://iarchive-stability/georgeblood/150370.tar -']\ndataset = wds.WebDataset(urls).decode(wds.torch_audio).shuffle(10)\nfor sample in dataset:  \n    for k,v in sample.items():  # print the all entries in dict\n        print(f\"{k:20s} {repr(v)[:75]}\")\n    break\n\n__key__              '78_dickies-dream_lester-young-count-basies-kansas-city-seven-lester-young-\n__url__              'pipe:aws s3 cp s3://iarchive-stability/georgeblood/150370.tar -'\nflac                 (tensor([[-0.0006, -0.0006, -0.0006,  ...,  0.0018,  0.0039,  0.0043],\n    \ntif                  b'II*\\x00\\x08\\x00\\x00\\x00\\x19\\x00\\x00\\x01\\x03\\x00\\x01\\x00\\x00\\x00\\xd8\\x08\\x\nxml                  b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<metadata>\\n  <identifier>78_dick\n\n\nHere’s a utility routine we’ll use further down:\n\nsource\n\n\nfix_double_slashes\n\n fix_double_slashes (s, debug=False)\n\naws is pretty unforgiving compared to ‘normal’ filesystems. so here’s some ‘cleanup’\nTest that:\n\ns = 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai//datasets//ekto/1//ekto-000006.tar -'\nprint(fix_double_slashes(s, debug=True))\ns = 'hey/ho//lets/go'\nprint(fix_double_slashes(s, debug=True))\n\npipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000006.tar -\nhey/ho/lets/go\n\n\n\n\nNon-contiguously-numbered lists of tar files…\nHere we’ll just get individual URLs for every tar file possible for a given list of dataset names\n\nsource\n\n\nget_all_s3_urls\n\n get_all_s3_urls (names=['FSD50K'], subsets=[''],\n                  s3_url_prefix='s3://s-laion-audio/webdataset_tar/',\n                  recursive=True, filter_str='tar', debug=False)\n\nget urls of shards (tar files) for multiple datasets in one s3 bucket\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnames\nlist\n[‘FSD50K’]\nlist of all valid [LAION AudioDataset] dataset names\n\n\nsubsets\nlist\n[’’]\nlist of subsets you want from those datasets, e.g. [‘train’,‘valid’]\n\n\ns3_url_prefix\nstr\ns3://s-laion-audio/webdataset_tar/\nprefix for those\n\n\nrecursive\nbool\nTrue\nrecursively list all tar files in all subdirs\n\n\nfilter_str\nstr\ntar\nonly grab files with this substring\n\n\ndebug\nbool\nFalse\nprint debugging info – note: info displayed likely to change at dev’s whims\n\n\n\n\nurls = get_all_s3_urls(names=['ekto/1'], recursive=True, s3_url_prefix='s3://s-harmonai/datasets/', debug=False) # CJ's new dataset\nurls[0:10]  # let's just print 10 of them instead of the whole thing\n\n['pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000000.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000001.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000002.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000003.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000004.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000005.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000006.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000007.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000008.tar -',\n 'pipe:aws s3 --cli-connect-timeout 0 cp s3://s-harmonai/datasets/ekto/1/ekto-000009.tar -']\n\n\n\nurls = get_all_s3_urls(names=['georgeblood'], s3_url_prefix='s3://iarchive-stability/', subsets=[])\nlen(urls)\n\n32006"
  },
  {
    "objectID": "datasets.html#hybridaudiodataset",
    "href": "datasets.html#hybridaudiodataset",
    "title": "datasets",
    "section": "HybridAudioDataset",
    "text": "HybridAudioDataset\nCombines local paths and WebDataset (s3:) datasets, streaming. Recommend leaving local_paths blank though, because underlying torch.datasets.ChainDataset won’t let you randomly sample between local & web parts.\n\nsource\n\nHybridAudioDataset\n\n HybridAudioDataset (local_paths:list, webdataset_names:list,\n                     sample_rate=48000, sample_size=65536,\n                     random_crop=True, load_frac=1.0,\n                     cache_training_data=False, num_gpus=8,\n                     redraw_silence=True, silence_thresh=-60,\n                     max_redraws=2, augs='Stereo(), PhaseFlipper()',\n                     verbose=False, subsets=[],\n                     s3_url_prefixes=['s3://s-laion-\n                     audio/webdataset_tar/', 's3://iarchive-stability/',\n                     's3://s-harmonai/datasets/'], recursive=True,\n                     debug=False)\n\nCombines AudioDataset and WebDataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlocal_paths\nlist\n\nlist of local paths names to draw audio files from (recursively)\n\n\nwebdataset_names\nlist\n\nlist of LAION Audiodataset or Internet Archive dataset names\n\n\nsample_rate\nint\n48000\naudio sample rate in Hz\n\n\nsample_size\nint\n65536\nhow many audio samples in each “chunk”\n\n\nrandom_crop\nbool\nTrue\ntake chunks from random positions within files\n\n\nload_frac\nfloat\n1.0\nfraction of total dataset to load\n\n\ncache_training_data\nbool\nFalse\nTrue = pre-load whole dataset into memory (not fully supported)\n\n\nnum_gpus\nint\n8\nused only when cache_training_data=True, to avoid duplicates,\n\n\nredraw_silence\nbool\nTrue\na chunk containing silence will be replaced with a new one\n\n\nsilence_thresh\nint\n-60\nthreshold in dB below which we declare to be silence\n\n\nmax_redraws\nint\n2\nwhen redrawing silences, don’t do it more than this many\n\n\naugs\nstr\nStereo(), PhaseFlipper()\nlist of augmentation transforms after PadCrop, as a string\n\n\nverbose\nbool\nFalse\nwhether to print notices of reasampling or not\n\n\nsubsets\nlist\n[]\ncan specify, e.g. [‘train’,‘valid’] to exclude ‘test’. default= grab everything!\n\n\ns3_url_prefixes\nlist\n[‘s3://s-laion-audio/webdataset_tar/’, ‘s3://iarchive-stability/’, ‘s3://s-harmonai/datasets/’]\n\n\n\nrecursive\nbool\nTrue\ngrab all tar files (“shards”) recursively\n\n\ndebug\nbool\nFalse\nprint debugging info\n\n\n\nTest that:\n\nmy_ds = HybridAudioDataset(['/fsx/shawley/data/maestro'], ['FSD50K','georgeblood'], subsets=['train'])\nsample = next(iter(my_ds))\nplayable_spectrogram(sample, specs='wave_mel', output_type='live')\n\naugs = None\nAudioDataset:1276 files found.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nsample = next(iter(my_ds))\nplayable_spectrogram(sample, specs='wave_mel', output_type='live')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nNotice how these are both piano? Thats because… torch.data.ChainDatasets won’t randomly sample across all datasets, rather in proceeds through them in sequential order – which, maybe you want that! But otherwise, I (Hawley) recommend calling HybridAudioDataset with no local paths. As in, let’s try to the ekto dataset:\n\nmy_ds = HybridAudioDataset([], ['ekto/1'], debug=False)\nsample = next(iter(my_ds))\nplayable_spectrogram(sample, specs='wave_mel', output_type='live')"
  },
  {
    "objectID": "datasets.html#quickwebdataloader",
    "href": "datasets.html#quickwebdataloader",
    "title": "datasets",
    "section": "QuickWebDataLoader",
    "text": "QuickWebDataLoader\nAnother option in case HybridAudioDataset is too clunky or massive. This is a “quickie” setup that may work in many cases.\n\nsource\n\nwds_preprocess\n\n wds_preprocess (sample, sample_size=65536, sample_rate=48000,\n                 verbose=False)\n\nutility routine for QuickWebDataLoader, below\n\nsource\n\n\nQuickWebDataLoader\n\n QuickWebDataLoader (names=['ekto/1'], sample_size=65536,\n                     sample_rate=48000, num_workers=4, batch_size=4,\n                     audio_file_ext='flac', shuffle_vals=[1000, 10000],\n                     epoch_len=1000, debug=False, verbose=False,\n                     callback=<function wds_preprocess>, **kwargs)\n\nMinimal/quick implementation: Sets up a WebDataLoader with some typical defaults\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnames\nlist\n[‘ekto/1’]\nnames of datasets. will search laion, harmonai & IA s3 buckets for these\n\n\nsample_size\nint\n65536\nhow long each sample to grab via PadCrop\n\n\nsample_rate\nint\n48000\nstandard sr in Hz\n\n\nnum_workers\nint\n4\nin the PyTorch DataLoader sense\n\n\nbatch_size\nint\n4\ntypical batch size\n\n\naudio_file_ext\nstr\nflac\nyep this one only supports one extension at a time. try HybridAudioDataset for more\n\n\nshuffle_vals\nlist\n[1000, 10000]\nvalues passed into shuffle as per WDS tutorials\n\n\nepoch_len\nint\n1000\nhow many passes/loads make for an epoch? wds part of this is not well documented IMHO\n\n\ndebug\nbool\nFalse\nprint info on internal workings\n\n\nverbose\nbool\nFalse\nnot quite the same as debug. print things like notices of resampling\n\n\ncallback\nfunction\nwds_preprocess\nfunction to call for additional user-based processing\n\n\nkwargs\n\n\n\n\n\n\n\ntrain_dl = QuickWebDataLoader(names='FSD50K', debug=False, verbose=True)\naudio_batch = next(iter(train_dl))[0].squeeze()\nprint(\"audio_batch.shape = \",audio_batch.shape)\nplayable_spectrogram(audio_batch[0], specs='wave_mel', output_type='live')\n\nNote: 'Broken pipe' messages you might get aren't a big deal, but may indicate files that are too big.\naudio_batch.shape =  torch.Size([4, 2, 65536])\n\n\ndownload failed: s3://s-laion-audio/webdataset_tar/FSD50K/train/31.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FSD50K/train/5.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FSD50K/train/22.tar to - [Errno 32] Broken pipe\ndownload failed: s3://s-laion-audio/webdataset_tar/FSD50K/train/63.tar to - [Errno 32] Broken pipe"
  }
]